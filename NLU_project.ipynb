{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural language understanding from traditional methods to large language models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intent classification (IC) and slot-labeling (SL) are two main key components of the natural language understanding (NLU) block in a dialogue system.\n",
    "\n",
    "There is dependency between the intent type (eg: play music) and possible slots (eg: artist name, song, genre). Traditional systems often use a cascaded approach or a naive joint approach, where the error in intent classification affects the slot labeling task.\n",
    "\n",
    "A proper way of addressing both the tasks can help improve the performance and also efficiency of the dialogue systems. Instruction-tuned large language models are able to do the task jointly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this project is to compare traditional methods to the more recent large language model based methods for NLU. \n",
    "\n",
    "- Implement IC and SL using methods based on word-embeddings (eg: word2vec or glove).\n",
    "- Implement IC and SL by fine-tuning a pre-trained language model (eg: BERT or T5)\n",
    "- Implement IC and SL using incontext-learning without any finetuning (eg: OLMo-7B-Instruct, or Gemma-2B-Instruct).\n",
    "- Run all the above experiments on two standard datasets (eg: NLU-evaluation benchmark, SNIPS, Banking77).\n",
    "- Compare all the systems, analyze the results and summarize your findings.\n",
    "\n",
    "**References**\n",
    "* Liu et al Benchmarking Natural Language Understanding Services for building Conversational Agents\n",
    "* Dataset: NLU-Evaluation Benchmark\n",
    "* Weld et al A survey of joint intent detection and slot-filling models in natural language understanding\n",
    "* Han et al Bi-directional Joint Neural Networks for Intent Classification and Slot Filling\n",
    "* hugging-face transformers for pre-trained models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*First of all you need to specify the directory where you want to download nltk data if you haven't already.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "download_dir = '' # your `nltk_data` download directory\n",
    "\n",
    "nltk.download('punkt_tab', download_dir=download_dir)\n",
    "nltk.data.path.append(download_dir)\n",
    "\n",
    "file_path = 'NLU-Evaluation-Data-master/AnnotatedData/NLU-Data-Home-Domain-Annotated-All.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(file):\n",
    "    data = pd.read_csv(file, delimiter=';')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_data(file_path)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns *\"intent\"*, *\"scenario\"* and *\"answer_annotation\"* are relevant for our **IC** and **SL** tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the different values of the *\"status\"* and *\"notes\"* columns because we observe that they are also containing **NaN** values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status_values = data['status'].unique()\n",
    "notes_values = data['notes'].unique()\n",
    "\n",
    "print('Status values:', status_values)\n",
    "print('Notes values:', notes_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status_values = data['status'].value_counts()\n",
    "notes_values = data['notes'].value_counts()\n",
    "\n",
    "print('Status values:', status_values)\n",
    "print('Notes values:', notes_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many **NaN** values are in those columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NaN_status_values = data['status'].isna().sum()\n",
    "NaN_notes_values = data['notes'].isna().sum()\n",
    "\n",
    "print('NaN status values:', NaN_status_values)\n",
    "print('NaN notes values:', NaN_notes_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Cleaning\n",
    "We can remove the rows with values that start with **'IRR_'** in the *\"status\"* column, as the utterance will be ignored by the post processing scripts. We also replace all the NaN values with empty strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[~data['status'].str.startswith('IRR_', na=False)]\n",
    "\n",
    "data['userid'] = data['userid'].fillna('1.0')\n",
    "data['scenario'] = data['scenario'].fillna('audio')\n",
    "data['intent'] = data['intent'].fillna('volume_mute')\n",
    "data['status'] = data['status'].fillna('')\n",
    "data['notes'] = data['notes'].fillna('')\n",
    "data['answer_normalised'] = data['answer_normalised'].fillna('stop')\n",
    "data['answer'] = data['answer'].fillna('stop')\n",
    "data['question'] = data['question'].fillna('Write what you would tell your PDA in the foll...')\n",
    "\n",
    "data['answerid'] = data.index\n",
    "\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = data['answer_normalised'] \n",
    "intents = data['intent']\n",
    "\n",
    "print(\"Sample Sentences:\")\n",
    "print(sentences.head())\n",
    "print(\"\\nSample Intents:\")\n",
    "print(intents.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also encode the labels, using the intents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "encoded_intents = label_encoder.fit_transform(intents)\n",
    "\n",
    "print(\"Intent Label Mapping:\")\n",
    "for i, label in enumerate(label_encoder.classes_):\n",
    "    print(f\"{i}: {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Intent Classification (IC)** and **Slot-Labeling (SL)** using Word2Vec embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are splitting a dataset into training, validation, and testing subsets using the ``train_test_split`` function from the *sklearn.model_selection* module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val_test, y_train, y_val_test = train_test_split(sentences, encoded_intents, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_val_test, y_val_test, test_size=0.5, random_state=42)\n",
    "\n",
    "print(f\"Training Samples:\", len(X_train), \"(70%)\")\n",
    "print(f\"Validation Samples:\",len(X_val), \"(15%)\")\n",
    "print(f\"Testing Samples:\", len(X_test), \"(15%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We tokenize text data from training, validation, and test datasets using the ``word_tokenize`` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tokens = X_train.apply(lambda x: word_tokenize(x.lower()))\n",
    "X_val_tokens = X_val.apply(lambda x: word_tokenize(x.lower()))\n",
    "X_test_tokens = X_test.apply(lambda x: word_tokenize(x.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.head())\n",
    "print()\n",
    "print(X_train_tokens.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we train the Word2Vec model on the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = Word2Vec(X_train_tokens.tolist(), vector_size=100, window=5, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By averaging word embeddings, we get the sentence-level embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_embedding(tokens, model):\n",
    "    vectors = [model.wv[word] for word in tokens if word in model.wv]\n",
    "    if vectors:\n",
    "        return np.mean(vectors, axis=0)\n",
    "    else:\n",
    "        return np.zeros(model.vector_size)\n",
    "    \n",
    "X_train_emb = np.array([get_sentence_embedding(tokens, w2v_model) for tokens in X_train_tokens])\n",
    "X_val_emb = np.array([get_sentence_embedding(tokens, w2v_model) for tokens in X_val_tokens])\n",
    "X_test_emb = np.array([get_sentence_embedding(tokens, w2v_model) for tokens in X_test_tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can train one or several models on the training data. Here we will use the **Logistic Regression** and the **Random Forest** classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "lr_model = LogisticRegression(max_iter=500, random_state=42)\n",
    "lr_model.fit(X_train_emb, y_train)\n",
    "\n",
    "# Random Forest\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train_emb, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X, y_true, dataset_name, labels, print_matrix=False, print_report=False):\n",
    "    y_pred = model.predict(X)\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "    print(f\"Accuracy on {dataset_name} set: {accuracy:.4f}\")\n",
    "\n",
    "    if print_report:\n",
    "        print(f\"Classification Report for {dataset_name} set:\")\n",
    "        print(classification_report(y_true, y_pred, target_names=labels))\n",
    "    \n",
    "    if print_matrix:\n",
    "        conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        ax = sns.heatmap(\n",
    "            conf_matrix,\n",
    "            annot=False,\n",
    "            cmap=\"viridis\",\n",
    "            fmt=\"d\", \n",
    "            xticklabels=labels,\n",
    "            yticklabels=labels,\n",
    "            cbar_kws={'label': 'Count'}\n",
    "        )\n",
    "\n",
    "        ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha=\"right\", fontsize=8)\n",
    "        ax.set_yticklabels(ax.get_yticklabels(), rotation=0, fontsize=8)\n",
    "\n",
    "        plt.xlabel(\"Predicted Label\", fontsize=12)\n",
    "        plt.ylabel(\"True Label\", fontsize=12)\n",
    "        plt.title(f\"Confusion Matrix for {dataset_name} data\", fontsize=14)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression validation and testing evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Logistic Regression:\")\n",
    "evaluate_model(lr_model, X_val_emb, y_val, \"Validation\", label_encoder.classes_)\n",
    "evaluate_model(lr_model, X_test_emb, y_test, \"Test\", label_encoder.classes_, print_matrix=True, print_report=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest validation and testing evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Random Forest:\")\n",
    "evaluate_model(rf_model, X_val_emb, y_val, \"Validation\", label_encoder.classes_)\n",
    "evaluate_model(rf_model, X_test_emb, y_test, \"Test\", label_encoder.classes_, print_matrix=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The general accuracy from the validation and test data goes from 0.46 to 0.51.\n",
    "\n",
    "On the different confusion matrices, we observe the diagonal that represents correct predictions, where the true label matches the predicted label. The brighter is the color, the higher is the accuracy for those classes.\n",
    "\n",
    "The other cells indicate misclassification. The brighter spots represent more errors than the darker spot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_intent(sentence, model, w2v_model, label_encoder):\n",
    "    tokens = word_tokenize(sentence.lower())\n",
    "    emb = get_sentence_embedding(tokens, w2v_model)\n",
    "    pred = model.predict([emb])\n",
    "    return label_encoder.inverse_transform(pred)[0]\n",
    "\n",
    "sentence = \"What is the weather in Paris?\"\n",
    "print(f\"Sentence: {sentence}\")\n",
    "print(f\"Predicted Intent: {predict_intent(sentence, lr_model, w2v_model, label_encoder)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Wake me up at 7 am\"\n",
    "print(f\"Sentence: {sentence}\")\n",
    "print(f\"Predicted Intent: {predict_intent(sentence, lr_model, w2v_model, label_encoder)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can predict the missing entities to complete the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_missing_entities(row, model, w2v_model, label_encoder):\n",
    "    if pd.isna(row['suggested_entities']):\n",
    "        tokens = word_tokenize(row['answer_normalised'].lower())\n",
    "        emb = get_sentence_embedding(tokens, w2v_model)\n",
    "        pred = model.predict([emb])\n",
    "        predicted_intent = label_encoder.inverse_transform(pred)[0]\n",
    "        return predicted_intent\n",
    "    return row['suggested_entities']\n",
    "\n",
    "data['suggested_entities'] = data.apply(lambda row: predict_missing_entities(row, lr_model, w2v_model, label_encoder), axis=1)\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also try to predict the scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario_encoder = LabelEncoder()\n",
    "encoded_scenarios = scenario_encoder.fit_transform(data['scenario'])\n",
    "\n",
    "print(\"Scenarios Label Mapping:\")\n",
    "for i, label in enumerate(scenario_encoder.classes_):\n",
    "    print(f\"{i}: {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scenario, X_val_test_scenario, y_train_scenario, y_val_test_scenario = train_test_split(sentences, encoded_scenarios, test_size=0.3, random_state=42)\n",
    "X_val_scenario, X_test_scenario, y_val_scenario, y_test_scenario = train_test_split(X_val_test_scenario, y_val_test_scenario, test_size=0.5, random_state=42)\n",
    "\n",
    "X_train_tokens_scenario = X_train_scenario.apply(lambda x: word_tokenize(x.lower()))\n",
    "X_val_tokens_scenario = X_val_scenario.apply(lambda x: word_tokenize(x.lower()))\n",
    "X_test_tokens_scenario = X_test_scenario.apply(lambda x: word_tokenize(x.lower()))\n",
    "\n",
    "X_train_emb_scenario = np.array([get_sentence_embedding(tokens, w2v_model) for tokens in X_train_tokens_scenario])\n",
    "X_val_emb_scenario = np.array([get_sentence_embedding(tokens, w2v_model) for tokens in X_val_tokens_scenario])\n",
    "X_test_emb_scenario = np.array([get_sentence_embedding(tokens, w2v_model) for tokens in X_test_tokens_scenario])\n",
    "\n",
    "lr_model_scenario = LogisticRegression(max_iter=500, random_state=42)\n",
    "lr_model_scenario.fit(X_train_emb_scenario, y_train_scenario)\n",
    "\n",
    "print(\"Logistic Regression (Scenario Prediction):\")\n",
    "evaluate_model(lr_model_scenario, X_val_emb_scenario, y_val_scenario, \"Validation\", scenario_encoder.classes_)\n",
    "evaluate_model(lr_model_scenario, X_test_emb_scenario, y_test_scenario, \"Test\", scenario_encoder.classes_, print_matrix=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's work on slot-labeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we extract the slots from the sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_slots(annotation):\n",
    "    slots = []\n",
    "    if isinstance(annotation, str):\n",
    "        entities = re.findall(r'\\[([^\\]]+)\\]', annotation)\n",
    "        for entity in entities:\n",
    "            slot_type, slot_value = entity.split(':')\n",
    "            slots.append((slot_type, slot_value))\n",
    "            # print(f\"Slot Type: {slot_type}, Slot Value: {slot_value}\")\n",
    "    return slots\n",
    "\n",
    "data['slots'] = data['answer_annotation'].apply(extract_slots)\n",
    "data[['answer_annotation', 'slots']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slots = data['slots']\n",
    "slots = slots.explode()\n",
    "slots = slots.apply(pd.Series)\n",
    "slots.columns = ['slot_type', 'slot_value']\n",
    "slots = slots.reset_index(drop=True)\n",
    "slots['slot_type'] = slots['slot_type'].str.strip()\n",
    "slot_types = slots['slot_type'].unique().tolist()\n",
    "print(\"Slot Types:\", slot_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then convert each word into BIO tagging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bio_labels(sentence, slots):\n",
    "    tokens = word_tokenize(sentence)\n",
    "    labels = ['O'] * len(tokens)\n",
    "    for slot_type, slot_value in slots:\n",
    "        slot_tokens = word_tokenize(slot_value)\n",
    "        slot_tokens_len = len(slot_tokens)\n",
    "        for i in range(len(tokens)):\n",
    "            if tokens[i:i+slot_tokens_len] == slot_tokens:\n",
    "                labels[i] = f\"B-{slot_type.strip()}\"\n",
    "                labels[i+1:i+slot_tokens_len] = [f\"I-{slot_type}\"] * (slot_tokens_len - 1)\n",
    "    return labels\n",
    "\n",
    "data['bio_labels'] = data.apply(lambda row: get_bio_labels(row['answer_normalised'], row['slots']), axis=1)\n",
    "data[['answer_normalised', 'slots', 'bio_labels']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "X = data['answer_normalised'].tolist()\n",
    "X = [word_tokenize(x.lower()) for x in X]\n",
    "y = data['bio_labels'].tolist()\n",
    "\n",
    "def align_bio_labels(tokens, bio_labels):\n",
    "\taligned_labels = ['O'] * len(tokens)\n",
    "\tlabel_index = 0\n",
    "\tfor i, token in enumerate(tokens):\n",
    "\t\tif label_index < len(bio_labels) and bio_labels[label_index] != 'O':\n",
    "\t\t\taligned_labels[i] = bio_labels[label_index]\n",
    "\t\t\tlabel_index += 1\n",
    "\t\telif label_index < len(bio_labels):\n",
    "\t\t\tlabel_index += 1\n",
    "\treturn aligned_labels\n",
    "\n",
    "aligned_bio_labels = align_bio_labels(X, y)\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "y_encoded = mlb.fit_transform(aligned_bio_labels)\n",
    "\n",
    "# bio_labels = mlb.classes_.tolist()\n",
    "\n",
    "print(\"BIO Labels Mapping:\")\n",
    "for i, label in enumerate(mlb.classes_):\n",
    "    print(f\"{i}: {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val_test, y_train, y_val_test = train_test_split(X, y_encoded, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_val_test, y_val_test, test_size=0.5, random_state=42)\n",
    "\n",
    "sl_w2v_model = Word2Vec(X_train, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "X_train_emb = np.array([get_sentence_embedding(tokens, sl_w2v_model) for tokens in X_train])\n",
    "X_val_emb = np.array([get_sentence_embedding(tokens, sl_w2v_model) for tokens in X_val])\n",
    "X_test_emb = np.array([get_sentence_embedding(tokens, sl_w2v_model) for tokens in X_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model = OneVsRestClassifier(LogisticRegression(max_iter=500, random_state=42))\n",
    "lr_model.fit(X_train_emb, y_train)\n",
    "\n",
    "print(\"Logistic Regression (Slot Labeling):\")\n",
    "evaluate_model(lr_model, X_val_emb, y_val, \"Validation\", mlb.classes_)\n",
    "evaluate_model(lr_model, X_test_emb, y_test, \"Test\", mlb.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model = OneVsRestClassifier(RandomForestClassifier(n_estimators=100, random_state=42))\n",
    "rf_model.fit(X_train_emb, y_train)\n",
    "\n",
    "print(\"Random Forest (Slot Labeling):\")\n",
    "evaluate_model(rf_model, X_val_emb, y_val, \"Validation\", mlb.classes_)\n",
    "evaluate_model(rf_model, X_test_emb, y_test, \"Test\", mlb.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_slots(sentence, model, w2v_model, mlb):\n",
    "    tokens = word_tokenize(sentence.lower())\n",
    "    emb = get_sentence_embedding(tokens, w2v_model)\n",
    "    pred = model.predict([emb])\n",
    "    pred_labels = mlb.inverse_transform(pred)[0]\n",
    "    return align_bio_labels(tokens, pred_labels)\n",
    "\n",
    "sentence = \"Wake me up at 5 am tomorrow\"\n",
    "print(f\"Sentence: {sentence}\")\n",
    "print(f\"Predicted Slots: {predict_slots(sentence, lr_model, sl_w2v_model, mlb)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Can you remind me to buy milk at 5 pm?\"\n",
    "print(f\"Sentence: {sentence}\")\n",
    "print(f\"Predicted Slots: {predict_slots(sentence, lr_model, sl_w2v_model, mlb)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"wake me up at nine am on friday\"\n",
    "print(f\"Sentence: {sentence}\")\n",
    "print(f\"Predicted Slots: {predict_slots(sentence, lr_model, sl_w2v_model, mlb)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's continue by fine-tuning a pre-trained language model like BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intent Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "X = data['answer_normalised']\n",
    "y = data['intent']\n",
    "\n",
    "ic_label_encoder = LabelEncoder()\n",
    "ic_encoded_intents = ic_label_encoder.fit_transform(y)\n",
    "\n",
    "X_train, X_val_test, y_train, y_val_test = train_test_split(X, ic_encoded_intents, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_val_test, y_val_test, test_size=0.5, random_state=42)\n",
    "\n",
    "X_train = X_train.reset_index(drop=True)\n",
    "y_train = pd.Series(y_train).reset_index(drop=True)\n",
    "X_val = X_val.reset_index(drop=True)\n",
    "y_val = pd.Series(y_val).reset_index(drop=True)\n",
    "X_test = X_test.reset_index(drop=True)\n",
    "y_test = pd.Series(y_test).reset_index(drop=True)\n",
    "\n",
    "# Dataset class for tokenized inputs\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        encoded = self.tokenizer(text, max_length=self.max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "        return {\"input_ids\": encoded[\"input_ids\"].squeeze(0), \"attention_mask\": encoded[\"attention_mask\"].squeeze(0), \"label\": torch.tensor(label, dtype=torch.long),}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.head())\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "ic_bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "ic_train_dataset = TextDataset(X_train, y_train, ic_bert_tokenizer, max_length=32)\n",
    "ic_val_dataset = TextDataset(X_val, y_val, ic_bert_tokenizer, max_length=32)\n",
    "\n",
    "ic_train_loader = DataLoader(ic_train_dataset, batch_size=16, shuffle=True)\n",
    "ic_val_loader = DataLoader(ic_val_dataset, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-trained BERT model\n",
    "ic_model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=len(ic_label_encoder.classes_))\n",
    "ic_model = ic_model.to(device)\n",
    "\n",
    "ic_optimizer = torch.optim.Adam(ic_model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def ic_train(model, dataloader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(dataloader, desc=\"Training\"):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Move inputs to GPU\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "# Validation function\n",
    "def ic_evaluate(model, dataloader, device):\n",
    "    model.eval()\n",
    "    preds, true_labels = [], []\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            # Move inputs to GPU\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"label\"].to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "            logits = outputs.logits\n",
    "            preds.extend(torch.argmax(logits, dim=1).cpu().numpy())\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    accuracy = accuracy_score(true_labels, preds)\n",
    "    return total_loss / len(dataloader), accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n",
    "    train_loss = ic_train(ic_model, ic_train_loader, ic_optimizer, device)\n",
    "    val_loss, val_accuracy = ic_evaluate(ic_model, ic_val_loader, device)\n",
    "    print(f\"Training Loss: {train_loss:.4f}\")\n",
    "    print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "    print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "ic_model.save_pretrained(\"fine_tuned_bert\")\n",
    "ic_bert_tokenizer.save_pretrained(\"fine_tuned_bert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ic_model = BertForSequenceClassification.from_pretrained(\"fine_tuned_bert\").to(device)\n",
    "ic_bert_tokenizer = BertTokenizer.from_pretrained(\"fine_tuned_bert\")\n",
    "\n",
    "test_dataset = TextDataset(X_test, y_test, ic_bert_tokenizer, max_length=128)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16)\n",
    "\n",
    "test_accuracy = ic_evaluate(ic_model, test_loader, device)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "def predict_intent_bert(sentence, model, tokenizer, label_encoder, device):\n",
    "    inputs = tokenizer(sentence, max_length=128, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "    inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    pred = torch.argmax(logits, dim=1).cpu().numpy()[0]\n",
    "    return label_encoder.inverse_transform([pred])[0]\n",
    "\n",
    "sentence = \"What is the weather in Paris?\"\n",
    "print(f\"Sentence: {sentence}\")\n",
    "print(f\"Predicted Intent: {predict_intent_bert(sentence, ic_model, ic_bert_tokenizer, ic_label_encoder, device)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Could you please turn the lights on?\"\n",
    "print(f\"Sentence: {sentence}\")\n",
    "print(f\"Predicted Intent: {predict_intent_bert(sentence, ic_model, ic_bert_tokenizer, ic_label_encoder, device)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slot-Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast, BertForTokenClassification\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Encode BIO labels\n",
    "bio_label_encoder = LabelEncoder()\n",
    "bio_label_encoder.fit([label for labels in data['bio_labels'] for label in labels])\n",
    "num_labels = len(bio_label_encoder.classes_)\n",
    "\n",
    "# Dataset class for token-level tasks\n",
    "class TokenDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        labels = self.labels[idx]\n",
    "\n",
    "        # Tokenize the text and align the labels\n",
    "        encoded = self.tokenizer(\n",
    "            text.split(),\n",
    "            is_split_into_words=True,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        word_ids = encoded.word_ids(batch_index=0)\n",
    "        label_ids = [-100] * len(word_ids)  # Initialize with -100 for ignored tokens\n",
    "\n",
    "        for i, word_id in enumerate(word_ids):\n",
    "            if word_id is not None:  # Skip special tokens like [CLS] and [SEP]\n",
    "                label_ids[i] = bio_label_encoder.transform([labels[word_id]])[0]\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": encoded[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": encoded[\"attention_mask\"].squeeze(0),\n",
    "            \"labels\": torch.tensor(label_ids, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Prepare data\n",
    "X = data['answer_normalised']\n",
    "y = data['bio_labels']  # Use the list of labels directly\n",
    "\n",
    "# Split data\n",
    "X_train, X_val_test, y_train, y_val_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_val_test, y_val_test, test_size=0.5, random_state=42)\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "train_dataset = TokenDataset(X_train.tolist(), y_train.tolist(), tokenizer, max_length=32)\n",
    "val_dataset = TokenDataset(X_val.tolist(), y_val.tolist(), tokenizer, max_length=32)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16)\n",
    "\n",
    "# Load pre-trained BERT model for token classification\n",
    "model = BertForTokenClassification.from_pretrained(\"bert-base-uncased\", num_labels=num_labels)\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Training function\n",
    "def train(model, dataloader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(dataloader, desc=\"Training\"):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate(model, dataloader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    preds, true_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            logits = outputs.logits.cpu().numpy()\n",
    "            label_ids = labels.cpu().numpy()\n",
    "\n",
    "            # Collect predictions and true labels (ignoring -100)\n",
    "            for i, label in enumerate(label_ids):\n",
    "                preds.extend(np.argmax(logits[i], axis=1)[label != -100])\n",
    "                true_labels.extend(label[label != -100])\n",
    "\n",
    "    accuracy = accuracy_score(true_labels, preds)\n",
    "    return total_loss / len(dataloader), accuracy\n",
    "\n",
    "# Training loop\n",
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n",
    "    train_loss = train(model, train_loader, optimizer, device)\n",
    "    val_loss, val_accuracy = evaluate(model, val_loader, device)\n",
    "    print(f\"Training Loss: {train_loss:.4f}\")\n",
    "    print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "    print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "# Save the model and tokenizer\n",
    "model.save_pretrained(\"slot_labeling_bert\")\n",
    "tokenizer.save_pretrained(\"slot_labeling_bert\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sl_model = BertForTokenClassification.from_pretrained(\"slot_labeling_bert\").to(device)\n",
    "sl_tokenizer = BertTokenizerFast.from_pretrained(\"slot_labeling_bert\")\n",
    "\n",
    "test_dataset = TokenDataset(X_test.tolist(), y_test.tolist(), sl_tokenizer, max_length=128)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16)\n",
    "\n",
    "test_loss, test_accuracy = evaluate(sl_model, test_loader, device)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "def predict_slots_bert(sentence, model, tokenizer, label_encoder, device):\n",
    "    tokens = word_tokenize(sentence.lower())\n",
    "    inputs = tokenizer(tokens, is_split_into_words=True, return_tensors=\"pt\")\n",
    "    inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    preds = torch.argmax(logits, dim=2).cpu().numpy()[0]\n",
    "    labels = label_encoder.inverse_transform(preds)\n",
    "    return align_bio_labels(tokens, labels)\n",
    "\n",
    "sentence = \"Wake me up at 5 am tomorrow\"\n",
    "print(f\"Sentence: {sentence}\")\n",
    "print(f\"Predicted Slots: {predict_slots_bert(sentence, sl_model, sl_tokenizer, bio_label_encoder, device)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement IC and SL using incontext-learning without any finetuning (e.g. Flan-T5-base from Google)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-base\", legacy_format=False)\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intent_prompt = \"\"\"\n",
    "Classify the intent of the following sentences:\n",
    "\n",
    "Text: Wake me up at 7 am\n",
    "Intent: set_alarm\n",
    "\n",
    "Text: What is the weather in Paris?\n",
    "Intent: get_weather\n",
    "\n",
    "Text: Play some jazz music\n",
    "Intent: play_music\n",
    "\n",
    "Text: Set a reminder for my meeting at 3 pm\n",
    "Intent: set_reminder\n",
    "\n",
    "Text: Turn off the lights\n",
    "Intent: control_lights\n",
    "\n",
    "Text: {}\n",
    "Intent:\n",
    "\"\"\"\n",
    "\n",
    "def predict_output_flanT5(sentence, prompt, model, tokenizer):\n",
    "    input_text = prompt.format(sentence)\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "    outputs = model.generate(input_ids, max_length=50)\n",
    "    intent = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return intent\n",
    "\n",
    "sentence = \"Book a table for two at the restaurant\"\n",
    "predicted_intent = predict_output_flanT5(sentence, intent_prompt, model, tokenizer)\n",
    "print(f\"Sentence: {sentence}\")\n",
    "print(f\"Predicted Intent: {predicted_intent}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_predictions = 0\n",
    "total_predictions = len(data)\n",
    "\n",
    "for i in range(total_predictions):\n",
    "    sentence = data['answer_normalised'][i]\n",
    "    actual_intent = data['intent'][i]\n",
    "    predicted_intent = predict_output_flanT5(sentence, intent_prompt, model, tokenizer)\n",
    "    \n",
    "    if predicted_intent == actual_intent:\n",
    "        correct_predictions += 1\n",
    "\n",
    "accuracy = correct_predictions / total_predictions\n",
    "print(f\"Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if by giving all the available intents, the accuracy will increase or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intents_list = ', '.join(set(data['intent'])).join(['[', ']'])\n",
    "print(intents_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intent_prompt = f\"\"\"\n",
    "Here are all the possible intents:\n",
    "Intents: {intents_list}\n",
    "\n",
    "Classify the intent of the following sentence:\n",
    "\n",
    "Text: {{}}\n",
    "Intent:\n",
    "\"\"\"\n",
    "\n",
    "correct_predictions = 0\n",
    "total_predictions = len(data)\n",
    "\n",
    "for i in range(total_predictions):\n",
    "    sentence = data['answer_normalised'][i]\n",
    "    actual_intent = data['intent'][i]\n",
    "    predicted_intent = predict_output_flanT5(sentence, intent_prompt, model, tokenizer)\n",
    "    \n",
    "    if predicted_intent == actual_intent:\n",
    "        correct_predictions += 1\n",
    "\n",
    "accuracy = correct_predictions / total_predictions\n",
    "print(f\"Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slot Labeling with Google Flan-T5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slot_prompt = \"\"\"\n",
    "Extract the slots from the following sentences:\n",
    "\n",
    "Text: Wake me up at 7 am\n",
    "Slots: [time: 7 am]\n",
    "\n",
    "Text: Book a table for two at the restaurant\n",
    "Slots: [number: two, location: restaurant]\n",
    "\n",
    "Text: Set a reminder for my meeting at 3 pm\n",
    "Slots: [event: meeting, time: 3 pm]\n",
    "\n",
    "Text: Turn off the lights in the living room\n",
    "Slots: [action: turn off, object: lights, location: living room]\n",
    "\n",
    "Text: {}\n",
    "Slots:\n",
    "\"\"\"\n",
    "\n",
    "sentence = \"Remind me to buy milk at 5 pm\"\n",
    "predicted_slots = predict_output_flanT5(sentence, slot_prompt, model, tokenizer)\n",
    "print(f\"Sentence: {sentence}\")\n",
    "print(f\"Predicted Slots: {predicted_slots}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_predictions = 0\n",
    "total_predictions = len(data)\n",
    "\n",
    "for i in range(total_predictions):\n",
    "    sentence = data['answer_normalised'][i]\n",
    "    actual_slots = data['bio_labels'][i]\n",
    "    predicted_slots = predict_output_flanT5(sentence, slot_prompt, model, tokenizer)\n",
    "    \n",
    "    if predicted_slots == actual_slots:\n",
    "        correct_predictions += 1\n",
    "\n",
    "accuracy = correct_predictions / total_predictions\n",
    "print(f\"Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = data.sample(5)\n",
    "for i, row in samples.iterrows():\n",
    "    sentence = row['answer_normalised']\n",
    "    actual_intent = row['intent']\n",
    "    actual_slots = row['bio_labels']\n",
    "    \n",
    "    predicted_intent = predict_output_flanT5(sentence, intent_prompt, model, tokenizer)\n",
    "    predicted_slots = predict_output_flanT5(sentence, slot_prompt, model, tokenizer)\n",
    "    \n",
    "    print(f\"Sentence: {sentence}\")\n",
    "    print(f\"Actual Intent: {actual_intent}\")\n",
    "    print(f\"Predicted Intent: {predicted_intent}\")\n",
    "    print(f\"Actual Slots: {actual_slots}\")\n",
    "    print(f\"Predicted Slots: {predicted_slots}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(slot_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Here are all the possible labels:\n",
    "Labels: {slot_types}\n",
    "\n",
    "Give a list of slots (labels in BIO format) corresponding to the following sentence:\n",
    "\n",
    "Example:\n",
    "Sentence: \"email dad how is the weather this week\"\n",
    "Predicted Slots: ['O', 'B-relation', 'O', 'O', 'O', 'O', 'B-date', 'I-date ']\n",
    "\n",
    "Text: {{}}\n",
    "Slots:\n",
    "\"\"\"\n",
    "\n",
    "correct_predictions = 0\n",
    "# total_predictions = len(data)\n",
    "total_predictions = 50\n",
    "\n",
    "for i in range(total_predictions):\n",
    "    sentence = data['answer_normalised'][i]\n",
    "    actual_slots = data['bio_labels'][i]\n",
    "    predicted_slots = predict_output_flanT5(sentence, prompt, model, tokenizer)\n",
    "    \n",
    "    if predicted_slots == actual_slots:\n",
    "        correct_predictions += 1\n",
    "\n",
    "accuracy = correct_predictions / total_predictions\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "samples = data.sample(5)\n",
    "for i, row in samples.iterrows():\n",
    "    sentence = row['answer_normalised']\n",
    "    actual_slots = row['bio_labels']\n",
    "    \n",
    "    predicted_slots = predict_output_flanT5(sentence, prompt, model, tokenizer)\n",
    "    \n",
    "    print(f\"Sentence: {sentence}\")\n",
    "    print(f\"Actual Slots: {actual_slots}\")\n",
    "    print(f\"Predicted Slots: {predicted_slots}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
